{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c72680",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\OneDrive\\Desktop\\E6data_project\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "import csv\n",
    "import uuid\n",
    "import random\n",
    "import streamlit as st\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import together\n",
    "from langchain_openai import ChatOpenAI\n",
    "import google.generativeai as genai\n",
    "import math\n",
    "import requests\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f8b4e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "12a12eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# ds = load_dataset(\"Dahoas/instruct-synthetic-prompt-responses\")\n",
    "# train_ds = ds[\"train\"]\n",
    "# df = train_ds.to_pandas()\n",
    "# df.to_csv(\"input.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e85e042",
   "metadata": {},
   "outputs": [],
   "source": [
    "JUDGE_RUBRIC_PROMPT = r\"\"\"\n",
    "You are an impartial expert evaluator. Your task is to evaluate an AI agent‚Äôs response\n",
    "against the given prompt and ground_truth (if available).\n",
    "\n",
    "You must return STRICTLY one JSON object following this schema:\n",
    "\n",
    "{\n",
    "  \"prompt_id\": \"<prompt_id>\",\n",
    "  \"scores\": {\n",
    "    \"instruction_following\": <-1 | 0 | 1>,\n",
    "    \"hallucination\": <-1 | 0 | 1>,\n",
    "    \"assumption_control\": <-1 | 0 | 1>,\n",
    "    \"coherence_accuracy\": <-1 | 0 | 1>\n",
    "  },\n",
    "  \"explanations\": {\n",
    "    \"instruction_following\": \"<1 concise sentence explanation>\",\n",
    "    \"hallucination\": \"<1 concise sentence explanation>\",\n",
    "    \"assumption_control\": \"<1 concise sentence explanation>\",\n",
    "    \"coherence_accuracy\": \"<1 concise sentence explanation>\"\n",
    "  },\n",
    "  \"total_score\": <float from -1.0 to 1.0 rounded to 2 decimals>\n",
    "}\n",
    "\n",
    "--------------------\n",
    "### Dimension Definitions & Scoring:\n",
    "\n",
    "- instruction_following: Measures whether the agent follows the explicit user instructions.  \n",
    "  - 1 = Perfectly followed instructions (format, count, constraints).  \n",
    "  - 0 = Partially followed, minor deviation.  \n",
    "  - -1 = Ignored or violated instructions.  \n",
    "\n",
    "- hallucination: Measures factual grounding and avoidance of fabricated content.  \n",
    "  - 1 = No fabricated/false claims (fully grounded in ground_truth if given).  \n",
    "  - 0 = Slight speculation or unverifiable claim, but mostly correct.  \n",
    "  - -1 = Contains major fabricated/false claims.  \n",
    "\n",
    "- assumption_control: Measures whether the agent avoids or properly qualifies unjustified assumptions.  \n",
    "  - 1 = No unjustified assumptions or assumptions clearly stated.  \n",
    "  - 0 = Some minor assumptions not stated but not harmful.  \n",
    "  - -1 = Many or major unjustified assumptions.  \n",
    "\n",
    "- coherence_accuracy: Measures clarity, logical flow, and factual correctness.  \n",
    "  - 1 = Clear, logically flowing, accurate.  \n",
    "  - 0 = Somewhat clear but contains redundancy or mild confusion.  \n",
    "  - -1 = Confusing, disorganized, or factually wrong.  \n",
    "\n",
    "--------------------\n",
    "\n",
    "### Total Score Calculation:\n",
    "\n",
    "- Compute **total_score** as the arithmetic mean of the 4 scores above.  \n",
    "  Example: (instruction_following + hallucination + assumption_control + coherence_accuracy) √∑ 4.  \n",
    "- The result must be a float between **-1.0 and 1.0**, **rounded to 2 decimals**.  \n",
    "- Do not invent other formulas.  \n",
    "- Examples:  \n",
    "  - If scores = {1, 1, 1, 1} ‚Üí total_score = 1.00  \n",
    "  - If scores = {1, 0, 0, 1} ‚Üí total_score = 0.50  \n",
    "  - If scores = {0, -1, 0, 1} ‚Üí total_score = 0.00  \n",
    "  - If scores = {-1, -1, -1, -1} ‚Üí total_score = -1.00  \n",
    "\n",
    "--------------------\n",
    "\n",
    "Now evaluate the following case and return STRICT JSON only.\n",
    "Prompt: '''{prompt}'''\n",
    "Agent Response: '''{agent_response}'''\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a223b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIza\n"
     ]
    }
   ],
   "source": [
    "# Fetch Gemini key safely\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "print(f\"{GEMINI_API_KEY[:4]}\")\n",
    "\n",
    "if not GEMINI_API_KEY:\n",
    "    raise ValueError(\"‚ùå GEMINI_API_KEY not found. Please set it in your environment or .env file.\")\n",
    "\n",
    "# Configure Gemini\n",
    "genai.configure(api_key=GEMINI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e75a6f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gsk_2\n",
      "sk-or\n"
     ]
    }
   ],
   "source": [
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")   \n",
    "print(f\"{GROQ_API_KEY[:5]}\")\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")   \n",
    "print(f\"{OPENROUTER_API_KEY[:5]}\")\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=OPENROUTER_API_KEY,  # <-- replace with your key\n",
    ")\n",
    "model_1 = \"mistralai/mistral-7b-instruct:free\"\n",
    "model_2 = \"mistralai/mistral-small-24b-instruct-2501:free\"\n",
    "model_3 = \"google/gemma-3-27b-it:free\"\n",
    "model_4 = \"mistralai/mistral-small-3.1-24b-instruct:free\"\n",
    "model_5 = \"meta-llama/llama-4-maverick:free\"\n",
    "model_GROQ1 = \"llama-3.1-8b-instant\"\n",
    "model_GROQ2 = \"meta-llama/llama-4-maverick-17b-128e-instruct\"\n",
    "model_GROQ3 = \"openai/gpt-oss-120b\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f9650d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== SCORING FUNCTION (Qwen Judge) ====\n",
    "def score_with_qwen(prompt: str, response: str) -> dict:\n",
    "    try:\n",
    "        full_prompt = JUDGE_RUBRIC_PROMPT + \"\\n\\n\"\n",
    "        full_prompt += f\"Prompt: '''{prompt}'''\\n\\n\"\n",
    "        full_prompt += f\"Agent response: '''{response}'''\\n\\n\"\n",
    "        full_prompt += \"Output EXACT JSON now.\"\n",
    "\n",
    "        completion = client.chat.completions.create(\n",
    "            model=model_1,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an evaluator that strictly outputs JSON only.\"},\n",
    "                {\"role\": \"user\", \"content\": full_prompt},\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        raw_text = completion.choices[0].message.content.strip()\n",
    "\n",
    "        # Clean JSON (remove extra text around it)\n",
    "        if \"{\" in raw_text:\n",
    "            json_str = raw_text[raw_text.find(\"{\"): raw_text.rfind(\"}\") + 1]\n",
    "        else:\n",
    "            json_str = raw_text\n",
    "\n",
    "        return json.loads(json_str)\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"scores\": {}, \"total_score\": 0, \"error\": str(e)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0096020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_with_groq(prompt: str, response: str) -> dict:\n",
    "    try:\n",
    "        groq_key = os.environ.get(\"GROQ_API_KEY\")\n",
    "        if not groq_key:\n",
    "            return {\"scores\": {}, \"total_score\": 0, \"error\": \"Groq API key not configured\"}\n",
    "\n",
    "        url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "        headers = {\"Authorization\": f\"Bearer {groq_key}\"}\n",
    "        \n",
    "        full_prompt = JUDGE_RUBRIC_PROMPT + \"\\n\\n\"\n",
    "        full_prompt += f\"Prompt: '''{prompt}'''\\n\\n\"\n",
    "        full_prompt += f\"Agent response: '''{response}'''\\n\\n\"\n",
    "        full_prompt += \"Output EXACT JSON now.\"\n",
    "\n",
    "        payload = {\n",
    "            \"model\": model_GROQ1,   # üîë replace with Groq model you want\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are an evaluator that strictly outputs JSON only.\"},\n",
    "                {\"role\": \"user\", \"content\": full_prompt},\n",
    "            ],\n",
    "            \"temperature\": 0.0,\n",
    "            \"max_tokens\": 512,\n",
    "        }\n",
    "\n",
    "        resp = requests.post(url, headers=headers, json=payload, timeout=60)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "\n",
    "        raw_text = data[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "\n",
    "        # Extract clean JSON\n",
    "        if \"{\" in raw_text:\n",
    "            json_str = raw_text[raw_text.find(\"{\"): raw_text.rfind(\"}\") + 1]\n",
    "        else:\n",
    "            json_str = raw_text\n",
    "\n",
    "        return json.loads(json_str)\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"scores\": {}, \"total_score\": 0, \"error\": str(e)}    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2a45d2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== SCORING FUNCTION ====\n",
    "def score_with_gemini(prompt: str, response: str) -> dict:\n",
    "    try:\n",
    "        full_prompt = JUDGE_RUBRIC_PROMPT + \"\\n\\n\"\n",
    "        full_prompt += f\"Prompt: '''{prompt}'''\\n\\n\"\n",
    "        full_prompt += f\"Agent response: '''{response}'''\\n\\n\"\n",
    "        full_prompt += \"Output EXACT JSON now.\"\n",
    "\n",
    "        model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
    "        result = model.generate_content(full_prompt)\n",
    "\n",
    "        # Extract raw text\n",
    "        raw_text = \"\"\n",
    "        if result.candidates and result.candidates[0].content.parts:\n",
    "            raw_text = result.candidates[0].content.parts[0].text\n",
    "        else:\n",
    "            raw_text = result.text\n",
    "\n",
    "        # Clean JSON\n",
    "        json_str = raw_text.strip()\n",
    "        if \"{\" in json_str:\n",
    "            json_str = json_str[json_str.find(\"{\"): json_str.rfind(\"}\") + 1]\n",
    "\n",
    "        return json.loads(json_str)\n",
    "    except Exception as e:\n",
    "        return {\"scores\": {}, \"total_score\": 0, \"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3f1458ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "expla =[]\n",
    "def evaluate_csv(input_csv: str, output_csv: str, batch_size: int = 10, total_rows: int = 2):\n",
    "    df = pd.read_csv(input_csv)\n",
    "    if total_rows is None or total_rows > len(df):\n",
    "        total_rows = len(df)\n",
    "    print(f\"üìä Total rows to process: {total_rows}\")\n",
    "    # If file exists, resume from last completed batch\n",
    "    start_index = 0\n",
    "    if os.path.exists(output_csv):\n",
    "        done_df = pd.read_csv(output_csv)\n",
    "        start_index = len(done_df)\n",
    "        print(f\"‚è© Resuming from row {start_index}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch_start in range(start_index, total_rows, batch_size):\n",
    "        batch_end = min(batch_start + batch_size, total_rows)\n",
    "        batch = df.iloc[batch_start:batch_end]\n",
    "\n",
    "        results = []\n",
    "        for i, row in batch.iterrows():\n",
    "            prompt = row[\"prompt\"]\n",
    "            response = row[\"response\"]\n",
    "\n",
    "            judge = score_with_groq(prompt, response)  # <-- your function\n",
    "            scores = judge.get(\"scores\", {})\n",
    "            # total = judge.get(\"total_score\", 0)\n",
    "            print(judge)            \n",
    "\n",
    "            results.append({\n",
    "                \"prompt_id\": f\"Prompt_{i+1}\",\n",
    "                \"prompt\": prompt,\n",
    "                \"response\": response,\n",
    "                \"instruction_following\": scores.get(\"instruction_following\", 0),\n",
    "                \"hallucination\": scores.get(\"hallucination\", 0),\n",
    "                \"assumption_control\": scores.get(\"assumption_control\", 0),\n",
    "                \"coherence_accuracy\": scores.get(\"coherence_accuracy\", 0),\n",
    "                \"total_score\": np.mean([\n",
    "                    scores.get(\"instruction_following\", 0),\n",
    "                    scores.get(\"hallucination\", 0),\n",
    "                    scores.get(\"assumption_control\", 0),\n",
    "                    scores.get(\"coherence_accuracy\", 0)\n",
    "                ])\n",
    "            })\n",
    "\n",
    "        # Convert batch to DataFrame\n",
    "        out_df = pd.DataFrame(results)\n",
    "\n",
    "        # Append to CSV (no header after first batch)\n",
    "        if batch_start == 0 and not os.path.exists(output_csv):\n",
    "            out_df.to_csv(output_csv, index=False, mode=\"w\")\n",
    "        else:\n",
    "            out_df.to_csv(output_csv, index=False, mode=\"a\", header=False)\n",
    "\n",
    "        print(f\"‚úÖ Processed rows {batch_start+1}‚Äì{batch_end} / {total_rows}\")\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"üéØ Finished! Total time: {elapsed/60:.2f} minutes. Output saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ee25e99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Total rows to process: 2\n",
      "‚è© Resuming from row 0\n",
      "{'prompt_id': '1', 'scores': {'instruction_following': 1, 'hallucination': 1, 'assumption_control': 1, 'coherence_accuracy': 1}, 'explanations': {'instruction_following': \"The agent perfectly followed the user's instructions, providing a clear and detailed step-by-step guide to setting up a hydroponic garden for herbs.\", 'hallucination': 'The agent provided a factual and well-grounded response, without any fabricated or false claims.', 'assumption_control': 'The agent avoided making any unjustified assumptions and clearly stated the necessary steps for setting up a hydroponic garden for herbs.', 'coherence_accuracy': \"The agent's response was clear, logically flowing, and accurate, providing a well-organized and easy-to-follow guide.\"}, 'total_score': 1.0}\n",
      "{'prompt_id': 'What type of wine goes best with steak?', 'scores': {'instruction_following': 1, 'hallucination': 1, 'assumption_control': 1, 'coherence_accuracy': 1}, 'explanations': {'instruction_following': 'The agent perfectly followed the explicit user instruction to provide a wine pairing recommendation.', 'hallucination': 'The agent provided a well-grounded response with no fabricated or false claims.', 'assumption_control': 'The agent avoided making any unjustified assumptions and provided clear recommendations.', 'coherence_accuracy': \"The agent's response was clear, logically flowing, and factually accurate.\"}, 'total_score': 1.0}\n",
      "‚úÖ Processed rows 1‚Äì2 / 2\n",
      "üéØ Finished! Total time: 0.03 minutes. Output saved to output_scored.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    evaluate_csv(\"input.csv\", \"output_scored.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecadb59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
