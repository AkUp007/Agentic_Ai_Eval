{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "aacca06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import csv\n",
    "import uuid\n",
    "import random\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import re\n",
    "import pandas as pd\n",
    "import together\n",
    "from langchain_openai import ChatOpenAI\n",
    "import google.generativeai as genai\n",
    "import math\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict, Any, Optional\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2d1550c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")            \n",
    "OLLAMA_URL = os.getenv(\"OLLAMA_URL\")               \n",
    "GROQ_AVAILABLE = GROQ_API_KEY is not None  \n",
    "OLLAMA_AVAILABLE = OLLAMA_URL is not None\n",
    "os.environ[\"GEMINI_API_KEY\"] = \"GEMINI_API_KEY\"\n",
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "\n",
    "# Output CSV\n",
    "OUTPUT_CSV = \"agent_evaluation_results.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "47e31eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent list (6 agents). They will be treated as distinct 'agents' though we reuse the same model.\n",
    "AGENTS = [\n",
    "    {\"agent_id\": \"Agent_1\", \"agent_name\": \"llama-3.1-8b-instant\", \"backend\": \"groq\", \"model\": \"llama-3.1-8b-instant\"},\n",
    "    {\"agent_id\": \"Agent_2\", \"agent_name\": \"meta-llama/llama-4-maverick-17b-128e-instruct\", \"backend\": \"groq\", \"model\": \"meta-llama/llama-4-maverick-17b-128e-instruct\"},\n",
    "    {\"agent_id\": \"Agent_3\", \"agent_name\": \"openai/gpt-oss-120b\", \"backend\": \"groq\", \"model\": \"openai/gpt-oss-120b\"},\n",
    "    {\"agent_id\": \"Agent_4\", \"agent_name\": \"tinyllama\", \"backend\": \"ollama\", \"model\": \"tinyllama\"},\n",
    "    {\"agent_id\": \"Agent_5\", \"agent_name\": \"gemma:2b\", \"backend\": \"ollama\", \"model\": \"gemma:2b\"},\n",
    "    {\"agent_id\": \"Agent_6\", \"agent_name\": \"tinydolphin\", \"backend\": \"ollama\", \"model\": \"tinydolphin\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e5b69337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restriction: approximate 50-100 words. Convert to tokens roughly (conservative).\n",
    "# We'll set a max token budget to try to keep output in range.\n",
    "WORDS_MIN = 50\n",
    "WORDS_MAX = 100\n",
    "TOKENS_PER_WORD = 1.33  # approximate: tokens = words * 1.33 (conservative)\n",
    "MAX_TOKENS_FOR_RESPONSE = math.ceil(WORDS_MAX * TOKENS_PER_WORD)  # upper bound\n",
    "\n",
    "# Rate / concurrency\n",
    "PARALLEL_WORKERS = 4\n",
    "CALL_DELAY = 0.2\n",
    "MAX_TOKENS = 512  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2f362b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_with_groq(session, prompt: str, agent: Dict[str, str]) -> str:\n",
    "    \"\"\"\n",
    "    Generate response using Groq async call.\n",
    "    Falls back if error occurs.\n",
    "    \"\"\"\n",
    "    groq_key = os.environ.get(\"GROQ_API_KEY\") \n",
    "    if not groq_key:\n",
    "        return f\"[Groq not configured for {agent['agent_name']}]\"\n",
    "    \n",
    "    url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "    headers = {\"Authorization\": f\"Bearer {groq_key}\"}\n",
    "    payload = {\n",
    "        \"model\": agent[\"model\"], \n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": f\"{prompt}\\n\\nAnswer in {WORDS_MIN}-{WORDS_MAX} words.\"}\n",
    "        ],\n",
    "        \"max_tokens\": MAX_TOKENS,\n",
    "        \"temperature\": 0.7\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        async with session.post(url, headers=headers, json=payload, timeout=60) as resp:\n",
    "            data = await resp.json()\n",
    "            # Groq follows OpenAI schema: choices -> message -> content\n",
    "            return data[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "    except Exception as e:\n",
    "        return f\"[Groq exception: {e}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "82b88606",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_with_ollama(session, prompt: str, agent: Dict[str, str]) -> str:\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": agent[\"model\"],\n",
    "        \"prompt\": f\"{prompt}\\n\\nAnswer in {WORDS_MIN}-{WORDS_MAX} words.\",\n",
    "        \"stream\": True\n",
    "    }\n",
    "    try:\n",
    "        async with session.post(url, json=payload) as resp:\n",
    "            response_text = \"\"\n",
    "            async for line in resp.content:\n",
    "                if not line.strip():\n",
    "                    continue\n",
    "                try:\n",
    "                    data = json.loads(line.decode(\"utf-8\"))\n",
    "                    if \"response\" in data:\n",
    "                        response_text += data[\"response\"]\n",
    "                    if data.get(\"done\"):\n",
    "                        break\n",
    "                except Exception:\n",
    "                    continue\n",
    "            return response_text.strip()\n",
    "    except Exception as e:\n",
    "        return f\"[Ollama exception: {e}]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8da66206",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_response(session, prompt: str, agent: Dict[str, str]) -> str:\n",
    "    if agent[\"backend\"] == \"ollama\":\n",
    "        return await generate_with_ollama(session, prompt, agent)\n",
    "    elif agent[\"backend\"] == \"groq\":\n",
    "        return await generate_with_groq(session, prompt, agent)\n",
    "    return f\"[Unknown backend for {agent['agent_name']}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8a5ceb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "JUDGE_RUBRIC_PROMPT = \"\"\"\n",
    "You are an impartial expert evaluator. Your task is to evaluate an AI agentâ€™s response\n",
    "against the given prompt and ground_truth (if available).\n",
    "\n",
    "You must return STRICTLY one JSON object following this schema:\n",
    "\n",
    "{\n",
    "  \"agent_id\": \"<agent_id>\",\n",
    "  \"agent_name\": \"<agent_name>\",\n",
    "  \"prompt_id\": \"<prompt_id>\",\n",
    "  \"scores\": {\n",
    "    \"instruction_following\": <-1 | 0 | 1>,\n",
    "    \"hallucination\": <-1 | 0 | 1>,\n",
    "    \"assumption_control\": <-1 | 0 | 1>,\n",
    "    \"coherence_accuracy\": <-1 | 0 | 1>\n",
    "  },\n",
    "  \"explanations\": {\n",
    "    \"instruction_following\": \"<1 concise sentence explanation>\",\n",
    "    \"hallucination\": \"<1 concise sentence explanation>\",\n",
    "    \"assumption_control\": \"<1 concise sentence explanation>\",\n",
    "    \"coherence_accuracy\": \"<1 concise sentence explanation>\"\n",
    "  },\n",
    "  \"total_score\": <float from -1.0 to 1.0 rounded to 2 decimals>\n",
    "}\n",
    "\n",
    "--------------------\n",
    "### Dimension Definitions & Scoring:\n",
    "\n",
    "- instruction_following: Measures whether the agent follows the explicit user instructions.  \n",
    "  - 1 = Perfectly followed instructions (format, count, constraints).  \n",
    "  - 0 = Partially followed, minor deviation.  \n",
    "  - -1 = Ignored or violated instructions.  \n",
    "\n",
    "- hallucination: Measures factual grounding and avoidance of fabricated content.  \n",
    "  - 1 = No fabricated/false claims (fully grounded in ground_truth if given).  \n",
    "  - 0 = Slight speculation or unverifiable claim, but mostly correct.  \n",
    "  - -1 = Contains major fabricated/false claims.  \n",
    "\n",
    "- assumption_control: Measures whether the agent avoids or properly qualifies unjustified assumptions.  \n",
    "  - 1 = No unjustified assumptions or assumptions clearly stated.  \n",
    "  - 0 = Some minor assumptions not stated but not harmful.  \n",
    "  - -1 = Many or major unjustified assumptions.  \n",
    "\n",
    "- coherence_accuracy: Measures clarity, logical flow, and factual correctness.  \n",
    "  - 1 = Clear, logically flowing, accurate.  \n",
    "  - 0 = Somewhat clear but contains redundancy or mild confusion.  \n",
    "  - -1 = Confusing, disorganized, or factually wrong.  \n",
    "\n",
    "--------------------\n",
    "Now evaluate the following case and return STRICT JSON only.\n",
    "Prompt: '''{prompt}'''\n",
    "Ground Truth: '''{ground_truth}'''\n",
    "Agent Response: '''{agent_response}'''\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "49dee768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_with_gemini(prompt: str, response: str, ground_truth: str) -> Dict:\n",
    "    full_prompt = JUDGE_RUBRIC_PROMPT + \"\\n\\n\"\n",
    "    full_prompt += f\"Prompt: '''{prompt}'''\\n\\n\"\n",
    "    full_prompt += f\"Ground truth: '''{ground_truth}'''\\n\\n\" if ground_truth else \"Ground truth: None\\n\\n\"\n",
    "    full_prompt += f\"Agent response: '''{response}'''\\n\\n\"\n",
    "    full_prompt += \"Output EXACT JSON now.\"    \n",
    "    \n",
    "    try:\n",
    "      model = genai.GenerativeModel(\"gemini-1.5-flash\")  # safer default\n",
    "      result = model.generate_content(full_prompt)\n",
    "\n",
    "      # Grab raw text from Gemini\n",
    "      if result.candidates and result.candidates[0].content.parts:\n",
    "          raw_text = result.candidates[0].content.parts[0].text\n",
    "      else:\n",
    "          raw_text = result.text\n",
    "\n",
    "      # Try to extract JSON safely\n",
    "      json_str = raw_text.strip()\n",
    "      if \"{\" in json_str:\n",
    "          json_str = json_str[json_str.find(\"{\"): json_str.rfind(\"}\") + 1]\n",
    "\n",
    "      return json.loads(json_str)\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"scores\": {}, \"explanations\": {\"error\": f\"Gemini output parse failed: {e}\"}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "da856fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def simulate_judge(agent_record: Dict[str,str], ground_truth: Optional[str]) -> Dict[str,Any]:\n",
    "#     \"\"\"Deterministic simulation for judge outputs when Gemini is unavailable.\"\"\"\n",
    "#     resp = agent_record[\"response\"]\n",
    "#     aid = agent_record[\"agent_id\"]\n",
    "#     aname = agent_record[\"agent_name\"]\n",
    "#     pid = agent_record[\"prompt_id\"]\n",
    "\n",
    "#     seed = abs(hash(aid + resp)) % (2**31)\n",
    "#     rnd = random.Random(seed)\n",
    "#     # heuristics\n",
    "#     instr = 5 if \"-\" in resp or \"\\n\" in resp and resp.strip().startswith((\"-\", \"1\")) else 3\n",
    "#     hall = 5 if ground_truth and any(w.lower() in resp.lower() for w in ground_truth.split()[:6]) else rnd.randint(2,5)\n",
    "#     assumption = 5 if \"I assume\" in resp or \"assuming\" in resp else rnd.randint(2,5)\n",
    "#     coherence = 5 if len(resp.split()) >= 40 else rnd.randint(3,5)\n",
    "#     scores = {\n",
    "#         \"instruction_following\": int(max(1, min(5, instr))),\n",
    "#         \"hallucination\": int(max(1, min(5, hall))),\n",
    "#         \"assumption_control\": int(max(1, min(5, assumption))),\n",
    "#         \"coherence_accuracy\": int(max(1, min(5, coherence)))\n",
    "#     }\n",
    "#     total = round(sum(scores.values())/4.0, 2)\n",
    "#     explanations = {\n",
    "#         \"instruction_following\": \"Simulated: format looks like requested.\" if scores[\"instruction_following\"]>=4 else \"Simulated: format mismatch.\",\n",
    "#         \"hallucination\": \"Simulated: aligns with ground truth.\" if scores[\"hallucination\"]>=4 else \"Simulated: possible unsupported claims.\",\n",
    "#         \"assumption_control\": \"Simulated: assumptions minimal or stated.\" if scores[\"assumption_control\"]>=4 else \"Simulated: assumptions present without statement.\",\n",
    "#         \"coherence_accuracy\": \"Simulated: largely coherent.\" if scores[\"coherence_accuracy\"]>=4 else \"Simulated: somewhat terse.\"\n",
    "#     }\n",
    "#     return {\n",
    "#         \"agent_id\": aid,\n",
    "#         \"agent_name\": aname,\n",
    "#         \"prompt_id\": pid,\n",
    "#         \"scores\": scores,\n",
    "#         \"explanations\": explanations,\n",
    "#         \"total_score\": total,\n",
    "#         \"response\": resp,\n",
    "#         \"ground_truth\": ground_truth\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "edd087a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate_agents(prompt: str, ground_truth: str) -> List[Dict]:\n",
    "    results = []\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [generate_response(session, prompt, agent) for agent in AGENTS]\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "\n",
    "    for agent, response in zip(AGENTS, responses):\n",
    "        judge = score_with_gemini(prompt, response, ground_truth)\n",
    "        scores = judge.get(\"scores\", {})\n",
    "        explanations = judge.get(\"explanations\", {})\n",
    "        total = sum(scores.values()) / len(scores) if scores else 0\n",
    "        results.append({\n",
    "            \"agent_id\": agent[\"agent_id\"],\n",
    "            \"agent_name\": agent[\"agent_name\"],\n",
    "            \"prompt_id\": f\"Prompt_{uuid.uuid4().hex[:8]}\",\n",
    "            \"response\": response,\n",
    "            \"ground_truth\": ground_truth,\n",
    "            \"scores\": scores,\n",
    "            \"explanations\": explanations,\n",
    "            \"total_score\": total\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "dd779047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running multi-provider generation + Gemini judging pipeline...\n",
      "[\n",
      "  {\n",
      "    \"agent_id\": \"Agent_1\",\n",
      "    \"agent_name\": \"GroqAgent-1\",\n",
      "    \"prompt_id\": \"Prompt_955a50eb\",\n",
      "    \"response\": \"Here are three programming languages used for data science:\\n\\n1. Python\\n2. R\\n3. Julia\",\n",
      "    \"ground_truth\": \"Python, R, Julia\",\n",
      "    \"scores\": {\n",
      "      \"instruction_following\": 0,\n",
      "      \"hallucination\": 1,\n",
      "      \"assumption_control\": 1,\n",
      "      \"coherence_accuracy\": 1\n",
      "    },\n",
      "    \"explanations\": {\n",
      "      \"instruction_following\": \"The agent followed the instructions to list three languages but added extra formatting (numbers).\",\n",
      "      \"hallucination\": \"The agent did not hallucinate; the response is factually correct.\",\n",
      "      \"assumption_control\": \"The agent made no unjustified assumptions.\",\n",
      "      \"coherence_accuracy\": \"The response is coherent, accurate, and well-organized.\"\n",
      "    },\n",
      "    \"total_score\": 0.75\n",
      "  },\n",
      "  {\n",
      "    \"agent_id\": \"Agent_2\",\n",
      "    \"agent_name\": \"GroqAgent-2\",\n",
      "    \"prompt_id\": \"Prompt_359cb3d1\",\n",
      "    \"response\": \"Here are 3 programming languages used for data science: \\n1. Python\\n2. R\\n3. Julia. \\nThese languages are widely used for data analysis, machine learning, and visualization. They have extensive libraries and frameworks that support data science tasks. Many data scientists and analysts rely on these languages to extract insights from data.\",\n",
      "    \"ground_truth\": \"Python, R, Julia\",\n",
      "    \"scores\": {\n",
      "      \"instruction_following\": 0,\n",
      "      \"hallucination\": 1,\n",
      "      \"assumption_control\": 1,\n",
      "      \"coherence_accuracy\": 0\n",
      "    },\n",
      "    \"explanations\": {\n",
      "      \"instruction_following\": \"The agent followed the instruction to list three languages, but added an explanation which was explicitly forbidden.\",\n",
      "      \"hallucination\": \"The agent did not hallucinate any facts; the information added was true but unnecessary.\",\n",
      "      \"assumption_control\": \"No unjustified assumptions were made.\",\n",
      "      \"coherence_accuracy\": \"The response was coherent but included extra information violating the instructions.\"\n",
      "    },\n",
      "    \"total_score\": 0.5\n",
      "  },\n",
      "  {\n",
      "    \"agent_id\": \"Agent_2\",\n",
      "    \"agent_name\": \"GroqAgent-3\",\n",
      "    \"prompt_id\": \"Prompt_410ed21a\",\n",
      "    \"response\": \"\",\n",
      "    \"ground_truth\": \"Python, R, Julia\",\n",
      "    \"scores\": {\n",
      "      \"instruction_following\": -1,\n",
      "      \"hallucination\": 0,\n",
      "      \"assumption_control\": 0,\n",
      "      \"coherence_accuracy\": 0\n",
      "    },\n",
      "    \"explanations\": {\n",
      "      \"instruction_following\": \"The agent did not provide any response, thus failing to follow the instructions.\",\n",
      "      \"hallucination\": \"Since there was no response, there are no claims to evaluate for factuality.\",\n",
      "      \"assumption_control\": \"No assumptions were made due to the lack of a response.\",\n",
      "      \"coherence_accuracy\": \"There's nothing to assess for coherence or accuracy as the response is empty.\"\n",
      "    },\n",
      "    \"total_score\": -0.25\n",
      "  },\n",
      "  {\n",
      "    \"agent_id\": \"Agent_3\",\n",
      "    \"agent_name\": \"Ollama-Local-1\",\n",
      "    \"prompt_id\": \"Prompt_83a88749\",\n",
      "    \"response\": \"Here's an example:\\n\\nSure, here are three programming languages commonly used for data science:\\n\\n1. Python\\nPython is a widely-used language that is highly interoperable with other major programming languages. It's particularly well-suited for machine learning and statistical analysis due to its deep integration with libraries such as NumPy, SciPy, and Pandas. The language also offers efficient algorithms for solving optimization problems and building robust machine learning models.\\n\\n2. R\\nR is a software environment used specifically for data analysis and statistics. It's a powerful tool for creating visualizations, performing regression analyses, and conducting statistical tests. R provides extensive documentation and is easy to learn, thanks to its user-friendly syntax and a large collection of built-in functions.\\n\\n3. SQL (Structured Query Language)\\nSQL is a relational database management system commonly used for data manipulation in programming languages such as Python and R. It's especially useful when dealing with complex datasets that require data organization, filtering, and searching. SQL offers features like joins, subqueries, and complex queries to aid data analysis.\\n\\nThese are just a few examples of the many data science-related programming languages available in today's market. Depending on your specific needs and skillset, you may choose one or more depending on your project requirements and preferences.\",\n",
      "    \"ground_truth\": \"Python, R, Julia\",\n",
      "    \"scores\": {\n",
      "      \"instruction_following\": -1,\n",
      "      \"hallucination\": 0,\n",
      "      \"assumption_control\": 0,\n",
      "      \"coherence_accuracy\": 0\n",
      "    },\n",
      "    \"explanations\": {\n",
      "      \"instruction_following\": \"The agent ignored the instruction to list only 3 languages and not provide explanations.\",\n",
      "      \"hallucination\": \"The agent's descriptions of the languages are accurate, but it added extra information.\",\n",
      "      \"assumption_control\": \"The agent made the assumption that providing explanations would be helpful, despite explicit instructions not to do so.\",\n",
      "      \"coherence_accuracy\": \"The response is coherent but violates the instructions, making it inaccurate in following the prompt.\"\n",
      "    },\n",
      "    \"total_score\": -0.25\n",
      "  },\n",
      "  {\n",
      "    \"agent_id\": \"Agent_4\",\n",
      "    \"agent_name\": \"Ollama-Local-2\",\n",
      "    \"prompt_id\": \"Prompt_8f838ff2\",\n",
      "    \"response\": \"Sure, here are 3 programming languages commonly used in data science:\\n\\n1. Python\\n2. R\\n3. SQL\",\n",
      "    \"ground_truth\": \"Python, R, Julia\",\n",
      "    \"scores\": {\n",
      "      \"instruction_following\": 0,\n",
      "      \"hallucination\": 0,\n",
      "      \"assumption_control\": 1,\n",
      "      \"coherence_accuracy\": 1\n",
      "    },\n",
      "    \"explanations\": {\n",
      "      \"instruction_following\": \"The agent partially followed instructions by listing 3 languages, but added an unnecessary introduction.\",\n",
      "      \"hallucination\": \"While SQL is used in data science, it is debatable whether it's a primary language like Python or R; however, it's not entirely false.\",\n",
      "      \"assumption_control\": \"No unjustified assumptions were made.\",\n",
      "      \"coherence_accuracy\": \"The response is clear, accurate, and logically flows well.\"\n",
      "    },\n",
      "    \"total_score\": 0.5\n",
      "  },\n",
      "  {\n",
      "    \"agent_id\": \"Agent_4\",\n",
      "    \"agent_name\": \"Ollama-Local-3\",\n",
      "    \"prompt_id\": \"Prompt_510f3603\",\n",
      "    \"response\": \"Sure! Here's the list of programming languages that are commonly used for data science:\\n\\n1. Python - one of the most popular languages for data analysis and Machine Learning. It's also known as a great tool for data manipulation and exploration.\\n2. R - another popular choice for data scientists and statisticians. It is powerful, flexible, and has many built-in statistical functions.\\n3. Julia - an up-and-coming language that is catching up to Python's speed and performance in data science.\",\n",
      "    \"ground_truth\": \"Python, R, Julia\",\n",
      "    \"scores\": {\n",
      "      \"instruction_following\": -1,\n",
      "      \"hallucination\": 0,\n",
      "      \"assumption_control\": 0,\n",
      "      \"coherence_accuracy\": 0\n",
      "    },\n",
      "    \"explanations\": {\n",
      "      \"instruction_following\": \"The agent ignored the instruction to list only 3 languages and to not provide explanations.\",\n",
      "      \"hallucination\": \"The agent did not hallucinate; the information provided is factually accurate.\",\n",
      "      \"assumption_control\": \"The agent made the assumption that providing explanations would be helpful, despite the prompt's instructions.\",\n",
      "      \"coherence_accuracy\": \"The response is coherent but deviates from the instructions, making it less accurate.\"\n",
      "    },\n",
      "    \"total_score\": -0.25\n",
      "  }\n",
      "]\n",
      "CSV written to agent_evaluation_results.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === RUN ===\n",
    "if __name__ == \"__main__\":\n",
    "    # PROMPT = \"What are the ethical concerns of deploying AI systems in society?\"\n",
    "    # GROUND_TRUTH = \"Core concerns include algorithmic bias and fairness, data privacy and consent, transparency and accountability, and job displacement risks.\"\n",
    "    PROMPT = \"List exactly 3 programming languages used for data science. Do not explain.\"\n",
    "    GROUND_TRUTH = \"Python, R, Julia\"\n",
    "\n",
    "    print(\"Running multi-provider generation + Gemini judging pipeline...\")\n",
    "    # results = asyncio.run(evaluate_agents(PROMPT, GROUND_TRUTH))\n",
    "    results = await evaluate_agents(PROMPT, GROUND_TRUTH)\n",
    "    print(json.dumps(results, indent=2))\n",
    "\n",
    "    # Write CSV\n",
    "    rows = []\n",
    "    for r in results:\n",
    "        row = {\n",
    "            \"agent_id\": r[\"agent_id\"],\n",
    "            \"agent_name\": r[\"agent_name\"],\n",
    "            \"prompt_id\": r[\"prompt_id\"],\n",
    "            \"response\": r[\"response\"],\n",
    "            \"ground_truth\": r[\"ground_truth\"],\n",
    "            **{f\"{k}_score\": v for k, v in r[\"scores\"].items()},\n",
    "            \"total_score\": r[\"total_score\"]\n",
    "        }\n",
    "        rows.append(row)\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(\"agent_evaluation_results.csv\", index=False)\n",
    "    print(\"CSV written to agent_evaluation_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d23c59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7852b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
